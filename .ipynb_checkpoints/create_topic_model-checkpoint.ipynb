{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tech' 'entertainment' 'politics' 'sport' 'business']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>document_page_type</th>\n",
       "      <th>text</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dataset/bbc/tech/316.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>More women turn to net security\\n\\nOlder peopl...</td>\n",
       "      <td>316.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>dataset/bbc/tech/275.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Souped-up wi-fi is on the horizon\\n\\nSuper hig...</td>\n",
       "      <td>275.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>dataset/bbc/tech/020.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Security scares spark browser fix\\n\\nMicrosoft...</td>\n",
       "      <td>020.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dataset/bbc/tech/212.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Lifestyle 'governs mobile choice'\\n\\nFaster, b...</td>\n",
       "      <td>212.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>dataset/bbc/tech/253.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Fast lifts rise into record books\\n\\nTwo high-...</td>\n",
       "      <td>253.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id                 file_path document_page_type  \\\n",
       "0           0  1.0  dataset/bbc/tech/316.txt               tech   \n",
       "1           1  2.0  dataset/bbc/tech/275.txt               tech   \n",
       "2           2  3.0  dataset/bbc/tech/020.txt               tech   \n",
       "3           3  4.0  dataset/bbc/tech/212.txt               tech   \n",
       "4           4  5.0  dataset/bbc/tech/253.txt               tech   \n",
       "\n",
       "                                                text file_name  \n",
       "0  More women turn to net security\\n\\nOlder peopl...   316.txt  \n",
       "1  Souped-up wi-fi is on the horizon\\n\\nSuper hig...   275.txt  \n",
       "2  Security scares spark browser fix\\n\\nMicrosoft...   020.txt  \n",
       "3  Lifestyle 'governs mobile choice'\\n\\nFaster, b...   212.txt  \n",
       "4  Fast lifts rise into record books\\n\\nTwo high-...   253.txt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('dataset/class_text.csv')\n",
    "print(df.document_page_type.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['More women turn to net security Older people and women are increasingly '\n",
      " 'taking charge of protecting home computers against malicious net attacks, '\n",
      " 'according to a two-year study. The number of women buying programs to '\n",
      " 'protect PCs from virus, spam and spyware attacks rose by 11.2% each year '\n",
      " 'between 2002 and 2004. The study, for net security firm Preventon, shows '\n",
      " 'that security messages are reaching a diversity of surfers. It is thought '\n",
      " 'that 40% of those buying home net security programs are retired. For the '\n",
      " 'last three years, that has gone up by an average of 13.2%. But more retired '\n",
      " 'women (53%) were buying security software than retired men. The research '\n",
      " 'reflects the changing stereotype and demographics of web users, as well as '\n",
      " 'growing awareness of the greater risks that high-speed broadband net '\n",
      " 'connections can pose to surfers. The study predicts that 40% of all home PC '\n",
      " 'net security buyers will be women in 2005. They could even overtake men as '\n",
      " 'the main buyers by 2007, if current rates persist, according to the '\n",
      " 'research. \"I think older people have become more vigilant about protecting '\n",
      " 'their PCs as we tend to be more cautious and want an insurance policy in '\n",
      " 'case something does go wrong\", said one over-60 woman who took part in the '\n",
      " 'research. \"You started off with young male stereotype computer users for '\n",
      " 'last 10 years,\" Paul Goosens, head of Preventon told the BBC News website. '\n",
      " '\"Now we are seeing real people - both sexes and very often it is women who '\n",
      " 'have more access at home.\" But net service providers still need to take more '\n",
      " 'responsibility in making sure people are educated about net threats before '\n",
      " 'they go online, particuarly if they are new to broadband, he said. Programs '\n",
      " 'also need to be tailored so that they can be installed by dial-up users with '\n",
      " 'a slower connection too, said Mr Goosens. Security software should be easy '\n",
      " 'to use, with simple interfaces and instructions written in non-technical '\n",
      " 'language, he added. The nature of the security threats are also becoming '\n",
      " 'more than just about e-mail viruses. High-profile complaints about rogue '\n",
      " 'diallers, and spyware or other programs that surreptitiously install '\n",
      " 'themselves on computers, have also raised awareness about the need to have a '\n",
      " 'combination of anti-virus, firewall and spyware-removal programs too. '\n",
      " 'Without protection, these kinds of programs can be picked up just through '\n",
      " 'surfing the web normally. More than 30,000 PCs a day globally are being '\n",
      " 'recruited into networks that spread spam and viruses, a study from security '\n",
      " 'from Symantec showed last year. Viruses written to make headlines by '\n",
      " 'infecting millions are also getting rarer, according to net security '\n",
      " 'experts. Programs are being unleashed to directly profit criminal gangs, '\n",
      " 'many based in Eastern Europe, over those which are designed to show off '\n",
      " 'technical skills or cause nuisance. The research shows that more people are '\n",
      " 'taking these criminal net threats more seriously because, said Mr Goosens, '\n",
      " 'they are reported in the press much more. \"You are seeing older users being '\n",
      " 'educated by the media and are seeing them picking up on this threat. They '\n",
      " 'are asking the right questions,\" he explained. \"It is more likely the '\n",
      " 'younger users who naively assume that because they are using a reputable '\n",
      " 'service provider, that they are safe to connect to the net.\" An unprotected '\n",
      " 'computer on a broadband connection can be breached and infected with viruses '\n",
      " 'or spyware within minutes. By the end of the year it is thought that more '\n",
      " 'than 30% of UK homes will have broadband net access. In July last year, the '\n",
      " 'number of UK households accessing the net via broadband surpassed those '\n",
      " 'using dial-up for the first time, according to the Office of National '\n",
      " 'Statistics. ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.text.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "# data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['more', 'women', 'turn', 'to', 'net', 'security', 'older', 'people', 'and', 'women', 'are', 'increasingly', 'taking', 'charge', 'of', 'protecting', 'home', 'computers', 'against', 'malicious', 'net', 'attacks', 'according', 'to', 'two', 'year', 'study', 'the', 'number', 'of', 'women', 'buying', 'programs', 'to', 'protect', 'pcs', 'from', 'virus', 'spam', 'and', 'spyware', 'attacks', 'rose', 'by', 'each', 'year', 'between', 'and', 'the', 'study', 'for', 'net', 'security', 'firm', 'preventon', 'shows', 'that', 'security', 'messages', 'are', 'reaching', 'diversity', 'of', 'surfers', 'it', 'is', 'thought', 'that', 'of', 'those', 'buying', 'home', 'net', 'security', 'programs', 'are', 'retired', 'for', 'the', 'last', 'three', 'years', 'that', 'has', 'gone', 'up', 'by', 'an', 'average', 'of', 'but', 'more', 'retired', 'women', 'were', 'buying', 'security', 'software', 'than', 'retired', 'men', 'the', 'research', 'reflects', 'the', 'changing', 'stereotype', 'and', 'demographics', 'of', 'web', 'users', 'as', 'well', 'as', 'growing', 'awareness', 'of', 'the', 'greater', 'risks', 'that', 'high', 'speed', 'broadband', 'net', 'connections', 'can', 'pose', 'to', 'surfers', 'the', 'study', 'predicts', 'that', 'of', 'all', 'home', 'pc', 'net', 'security', 'buyers', 'will', 'be', 'women', 'in', 'they', 'could', 'even', 'overtake', 'men', 'as', 'the', 'main', 'buyers', 'by', 'if', 'current', 'rates', 'persist', 'according', 'to', 'the', 'research', 'think', 'older', 'people', 'have', 'become', 'more', 'vigilant', 'about', 'protecting', 'their', 'pcs', 'as', 'we', 'tend', 'to', 'be', 'more', 'cautious', 'and', 'want', 'an', 'insurance', 'policy', 'in', 'case', 'something', 'does', 'go', 'wrong', 'said', 'one', 'over', 'woman', 'who', 'took', 'part', 'in', 'the', 'research', 'you', 'started', 'off', 'with', 'young', 'male', 'stereotype', 'computer', 'users', 'for', 'last', 'years', 'paul', 'goosens', 'head', 'of', 'preventon', 'told', 'the', 'bbc', 'news', 'website', 'now', 'we', 'are', 'seeing', 'real', 'people', 'both', 'sexes', 'and', 'very', 'often', 'it', 'is', 'women', 'who', 'have', 'more', 'access', 'at', 'home', 'but', 'net', 'service', 'providers', 'still', 'need', 'to', 'take', 'more', 'responsibility', 'in', 'making', 'sure', 'people', 'are', 'educated', 'about', 'net', 'threats', 'before', 'they', 'go', 'online', 'particuarly', 'if', 'they', 'are', 'new', 'to', 'broadband', 'he', 'said', 'programs', 'also', 'need', 'to', 'be', 'tailored', 'so', 'that', 'they', 'can', 'be', 'installed', 'by', 'dial', 'up', 'users', 'with', 'slower', 'connection', 'too', 'said', 'mr', 'goosens', 'security', 'software', 'should', 'be', 'easy', 'to', 'use', 'with', 'simple', 'interfaces', 'and', 'instructions', 'written', 'in', 'non', 'technical', 'language', 'he', 'added', 'the', 'nature', 'of', 'the', 'security', 'threats', 'are', 'also', 'becoming', 'more', 'than', 'just', 'about', 'mail', 'viruses', 'high', 'profile', 'complaints', 'about', 'rogue', 'diallers', 'and', 'spyware', 'or', 'other', 'programs', 'that', 'surreptitiously', 'install', 'themselves', 'on', 'computers', 'have', 'also', 'raised', 'awareness', 'about', 'the', 'need', 'to', 'have', 'combination', 'of', 'anti', 'virus', 'firewall', 'and', 'spyware', 'removal', 'programs', 'too', 'without', 'protection', 'these', 'kinds', 'of', 'programs', 'can', 'be', 'picked', 'up', 'just', 'through', 'surfing', 'the', 'web', 'normally', 'more', 'than', 'pcs', 'day', 'globally', 'are', 'being', 'recruited', 'into', 'networks', 'that', 'spread', 'spam', 'and', 'viruses', 'study', 'from', 'security', 'from', 'symantec', 'showed', 'last', 'year', 'viruses', 'written', 'to', 'make', 'headlines', 'by', 'infecting', 'millions', 'are', 'also', 'getting', 'rarer', 'according', 'to', 'net', 'security', 'experts', 'programs', 'are', 'being', 'unleashed', 'to', 'directly', 'profit', 'criminal', 'gangs', 'many', 'based', 'in', 'eastern', 'europe', 'over', 'those', 'which', 'are', 'designed', 'to', 'show', 'off', 'technical', 'skills', 'or', 'cause', 'nuisance', 'the', 'research', 'shows', 'that', 'more', 'people', 'are', 'taking', 'these', 'criminal', 'net', 'threats', 'more', 'seriously', 'because', 'said', 'mr', 'goosens', 'they', 'are', 'reported', 'in', 'the', 'press', 'much', 'more', 'you', 'are', 'seeing', 'older', 'users', 'being', 'educated', 'by', 'the', 'media', 'and', 'are', 'seeing', 'them', 'picking', 'up', 'on', 'this', 'threat', 'they', 'are', 'asking', 'the', 'right', 'questions', 'he', 'explained', 'it', 'is', 'more', 'likely', 'the', 'younger', 'users', 'who', 'naively', 'assume', 'that', 'because', 'they', 'are', 'using', 'reputable', 'service', 'provider', 'that', 'they', 'are', 'safe', 'to', 'connect', 'to', 'the', 'net', 'an', 'unprotected', 'computer', 'on', 'broadband', 'connection', 'can', 'be', 'breached', 'and', 'infected', 'with', 'viruses', 'or', 'spyware', 'within', 'minutes', 'by', 'the', 'end', 'of', 'the', 'year', 'it', 'is', 'thought', 'that', 'more', 'than', 'of', 'uk', 'homes', 'will', 'have', 'broadband', 'net', 'access', 'in', 'july', 'last', 'year', 'the', 'number', 'of', 'uk', 'households', 'accessing', 'the', 'net', 'via', 'broadband', 'surpassed', 'those', 'using', 'dial', 'up', 'for', 'the', 'first', 'time', 'according', 'to', 'the', 'office', 'of', 'national', 'statistics']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.phrases.Phraser object at 0x7f478e51d7c0>\n",
      "['more', 'women', 'turn', 'to', 'net', 'security', 'older', 'people', 'and', 'women', 'are', 'increasingly', 'taking', 'charge', 'of', 'protecting', 'home', 'computers', 'against', 'malicious', 'net', 'attacks', 'according', 'to', 'two', 'year', 'study', 'the', 'number', 'of', 'women', 'buying', 'programs', 'to', 'protect', 'pcs', 'from', 'virus', 'spam', 'and', 'spyware', 'attacks', 'rose', 'by', 'each', 'year', 'between', 'and', 'the', 'study', 'for', 'net', 'security', 'firm', 'preventon', 'shows', 'that', 'security', 'messages', 'are', 'reaching', 'diversity', 'of', 'surfers', 'it', 'is', 'thought', 'that', 'of', 'those', 'buying', 'home', 'net', 'security', 'programs', 'are', 'retired', 'for', 'the', 'last', 'three', 'years', 'that', 'has', 'gone', 'up', 'by', 'an', 'average', 'of', 'but', 'more', 'retired', 'women', 'were', 'buying', 'security', 'software', 'than', 'retired', 'men', 'the', 'research', 'reflects', 'the', 'changing', 'stereotype', 'and', 'demographics', 'of', 'web', 'users', 'as', 'well', 'as', 'growing', 'awareness', 'of', 'the', 'greater', 'risks', 'that', 'high_speed', 'broadband', 'net', 'connections', 'can', 'pose', 'to', 'surfers', 'the', 'study', 'predicts', 'that', 'of', 'all', 'home', 'pc', 'net', 'security', 'buyers', 'will', 'be', 'women', 'in', 'they', 'could', 'even', 'overtake', 'men', 'as', 'the', 'main', 'buyers', 'by', 'if', 'current', 'rates', 'persist', 'according', 'to', 'the', 'research', 'think', 'older', 'people', 'have', 'become', 'more', 'vigilant', 'about', 'protecting', 'their', 'pcs', 'as', 'we', 'tend', 'to', 'be', 'more', 'cautious', 'and', 'want', 'an', 'insurance', 'policy', 'in', 'case', 'something', 'does', 'go', 'wrong', 'said', 'one', 'over', 'woman', 'who', 'took', 'part', 'in', 'the', 'research', 'you', 'started', 'off', 'with', 'young', 'male', 'stereotype', 'computer', 'users', 'for', 'last', 'years', 'paul', 'goosens', 'head', 'of', 'preventon', 'told', 'the', 'bbc_news_website', 'now', 'we', 'are', 'seeing', 'real', 'people', 'both', 'sexes', 'and', 'very', 'often', 'it', 'is', 'women', 'who', 'have', 'more', 'access', 'at', 'home', 'but', 'net', 'service_providers', 'still', 'need', 'to', 'take', 'more', 'responsibility', 'in', 'making_sure', 'people', 'are', 'educated', 'about', 'net', 'threats', 'before', 'they', 'go', 'online', 'particuarly', 'if', 'they', 'are', 'new', 'to', 'broadband', 'he', 'said', 'programs', 'also', 'need', 'to', 'be', 'tailored', 'so', 'that', 'they', 'can', 'be', 'installed', 'by', 'dial_up', 'users', 'with', 'slower', 'connection', 'too', 'said', 'mr', 'goosens', 'security', 'software', 'should', 'be', 'easy', 'to', 'use', 'with', 'simple', 'interfaces', 'and', 'instructions', 'written', 'in', 'non', 'technical', 'language', 'he', 'added', 'the', 'nature', 'of', 'the', 'security', 'threats', 'are', 'also', 'becoming', 'more', 'than', 'just', 'about', 'mail', 'viruses', 'high_profile', 'complaints', 'about', 'rogue', 'diallers', 'and', 'spyware', 'or', 'other', 'programs', 'that', 'surreptitiously', 'install', 'themselves', 'on', 'computers', 'have', 'also', 'raised', 'awareness', 'about', 'the', 'need', 'to', 'have', 'combination', 'of', 'anti_virus', 'firewall', 'and', 'spyware', 'removal', 'programs', 'too', 'without', 'protection', 'these', 'kinds', 'of', 'programs', 'can', 'be', 'picked_up', 'just', 'through', 'surfing', 'the', 'web', 'normally', 'more', 'than', 'pcs', 'day', 'globally', 'are', 'being', 'recruited', 'into', 'networks', 'that', 'spread', 'spam', 'and', 'viruses', 'study', 'from', 'security', 'from', 'symantec', 'showed', 'last', 'year', 'viruses', 'written', 'to', 'make', 'headlines', 'by', 'infecting', 'millions', 'are', 'also', 'getting', 'rarer', 'according', 'to', 'net', 'security', 'experts', 'programs', 'are', 'being', 'unleashed', 'to', 'directly', 'profit', 'criminal', 'gangs', 'many', 'based', 'in', 'eastern_europe', 'over', 'those', 'which', 'are', 'designed', 'to', 'show', 'off', 'technical', 'skills', 'or', 'cause', 'nuisance', 'the', 'research', 'shows', 'that', 'more', 'people', 'are', 'taking', 'these', 'criminal', 'net', 'threats', 'more', 'seriously', 'because', 'said', 'mr', 'goosens', 'they', 'are', 'reported', 'in', 'the', 'press', 'much', 'more', 'you', 'are', 'seeing', 'older', 'users', 'being', 'educated', 'by', 'the', 'media', 'and', 'are', 'seeing', 'them', 'picking_up', 'on', 'this', 'threat', 'they', 'are', 'asking', 'the', 'right', 'questions', 'he', 'explained', 'it', 'is', 'more', 'likely', 'the', 'younger', 'users', 'who', 'naively', 'assume', 'that', 'because', 'they', 'are', 'using', 'reputable', 'service_provider', 'that', 'they', 'are', 'safe', 'to', 'connect', 'to', 'the', 'net', 'an', 'unprotected', 'computer', 'on', 'broadband', 'connection', 'can', 'be', 'breached', 'and', 'infected', 'with', 'viruses', 'or', 'spyware', 'within', 'minutes', 'by', 'the', 'end', 'of', 'the', 'year', 'it', 'is', 'thought', 'that', 'more', 'than', 'of', 'uk', 'homes', 'will', 'have', 'broadband', 'net', 'access', 'in', 'july', 'last', 'year', 'the', 'number', 'of', 'uk', 'households', 'accessing', 'the', 'net', 'via', 'broadband', 'surpassed', 'those', 'using', 'dial_up', 'for', 'the', 'first', 'time', 'according', 'to', 'the', 'office', 'of', 'national_statistics']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod)\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['woman', 'turn', 'net', 'security', 'old', 'people', 'woman', 'increasingly', 'take', 'charge', 'protect', 'home', 'computer', 'malicious', 'net', 'attack', 'accord', 'year', 'study', 'number', 'woman', 'buy', 'program', 'protect', 'virus', 'spam', 'spyware', 'attack', 'rise', 'year', 'study', 'net', 'security', 'firm', 'preventon', 'show', 'security', 'message', 'reach', 'diversity', 'surfer', 'think', 'buy', 'home', 'net', 'security', 'program', 'retire', 'last', 'year', 'go', 'average', 'retired', 'woman', 'buy', 'security', 'software', 'retire', 'man', 'research', 'reflect', 'change', 'stereotype', 'demographic', 'web', 'user', 'well', 'grow', 'awareness', 'great', 'risk', 'broadband', 'net', 'connection', 'pose', 'surfer', 'study', 'predict', 'home', 'pc', 'net', 'security', 'buyer', 'woman', 'even', 'overtake', 'man', 'main', 'buyer', 'current', 'rate', 'persist', 'accord', 'research', 'think', 'old', 'people', 'become', 'vigilant', 'protect', 'tend', 'cautious', 'want', 'insurance', 'policy', 'case', 'go', 'wrong', 'say', 'woman', 'take', 'part', 'research', 'start', 'young', 'male', 'stereotype', 'computer', 'user', 'last', 'year', 'goosen', 'head', 'preventon', 'see', 'real', 'people', 'sex', 'often', 'woman', 'access', 'home', 'net', 'service_provider', 'still', 'need', 'take', 'responsibility', 'make', 'sure', 'people', 'educate', 'net', 'threat', 'go', 'online', 'particuarly', 'new', 'broadband', 'say', 'program', 'also', 'need', 'tailor', 'instal', 'dial', 'user', 'slow', 'connection', 'say', 'mr', 'goosen', 'security', 'software', 'easy', 'simple', 'interface', 'instruction', 'write', 'non', 'technical', 'language', 'add', 'nature', 'security', 'threat', 'also', 'become', 'mail', 'virus', 'high_profile', 'complaint', 'rogue', 'dialler', 'surreptitiously', 'install', 'computer', 'also', 'raise', 'awareness', 'need', 'combination', 'anti_virus', 'firewall', 'spyware', 'removal', 'program', 'protection', 'kind', 'program', 'pick', 'surfing', 'web', 'normally', 'pc', 'day', 'globally', 'recruit', 'network', 'spread', 'spam', 'virus', 'study', 'security', 'symantec', 'show', 'last', 'year', 'virus', 'write', 'make', 'headline', 'infect', 'million', 'also', 'get', 'rare', 'accord', 'net', 'security', 'expert', 'program', 'unleash', 'directly', 'profit', 'criminal', 'gang', 'many', 'base', 'design', 'show', 'technical', 'skill', 'cause', 'nuisance', 'research', 'show', 'people', 'take', 'criminal', 'net', 'threat', 'seriously', 'say', 'mr', 'goosen', 'report', 'press', 'much', 'see', 'old', 'user', 'educate', 'medium', 'see', 'pick', 'threat', 'ask', 'right', 'question', 'explain', 'likely', 'young', 'user', 'naively', 'assume', 'use', 'reputable', 'service_provider', 'safe', 'connect', 'net', 'unprotected', 'computer', 'broadband', 'connection', 'breach', 'infected', 'virus', 'spyware', 'minute', 'end', 'year', 'think', 'home', 'broadband', 'net', 'access', 'july', 'last', 'year', 'number', 'uk', 'household', 'access', 'net', 'broadband', 'surpass', 'use', 'dial', 'first', 'time', 'accord', 'office', 'national_statistic']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "# import sys\n",
    "# !{sys.executable} -m pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "import spacy as spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 3), (1, 4), (2, 1), (3, 4), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 2), (10, 1), (11, 2), (12, 1), (13, 5), (14, 3), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 4), (24, 1), (25, 3), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (48, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 5), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 4), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (82, 1), (83, 1), (84, 1), (85, 3), (86, 13), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 2), (93, 1), (94, 1), (95, 3), (96, 1), (97, 1), (98, 1), (99, 1), (100, 2), (101, 5), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 2), (109, 1), (110, 6), (111, 3), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 4), (125, 1), (126, 2), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 4), (134, 10), (135, 3), (136, 1), (137, 2), (138, 1), (139, 4), (140, 1), (141, 1), (142, 1), (143, 2), (144, 2), (145, 1), (146, 3), (147, 1), (148, 2), (149, 1), (150, 4), (151, 1), (152, 2), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 4), (159, 2), (160, 1), (161, 3), (162, 4), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (169, 5), (170, 1), (171, 5), (172, 1), (173, 2), (174, 1), (175, 7), (176, 2), (177, 1), (178, 7), (179, 2)]]\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.057*\"say\" + 0.040*\"mr\" + 0.033*\"government\" + 0.016*\"dollar\" + '\n",
      "  '0.014*\"economic\" + 0.013*\"oil\" + 0.012*\"state\" + 0.012*\"claim\" + '\n",
      "  '0.011*\"rule\" + 0.010*\"minister\"'),\n",
      " (1,\n",
      "  '0.039*\"holiday\" + 0.028*\"mail\" + 0.025*\"internet\" + 0.023*\"crude\" + '\n",
      "  '0.022*\"outlook\" + 0.020*\"online\" + 0.017*\"computer\" + 0.015*\"security\" + '\n",
      "  '0.014*\"acquisition\" + 0.014*\"software\"'),\n",
      " (2,\n",
      "  '0.042*\"say\" + 0.017*\"firm\" + 0.017*\"company\" + 0.016*\"new\" + 0.012*\"make\" + '\n",
      "  '0.011*\"deal\" + 0.011*\"offer\" + 0.011*\"also\" + 0.010*\"pay\" + 0.008*\"people\"'),\n",
      " (3,\n",
      "  '0.017*\"noise\" + 0.010*\"blast\" + 0.008*\"jim\" + 0.006*\"boat\" + 0.006*\"demise\" '\n",
      "  '+ 0.005*\"warner\" + 0.005*\"rd\" + 0.004*\"blade\" + 0.004*\"partisan\" + '\n",
      "  '0.004*\"menu\"'),\n",
      " (4,\n",
      "  '0.173*\"airline\" + 0.080*\"flight\" + 0.058*\"saturday\" + 0.039*\"employee\" + '\n",
      "  '0.038*\"suspend\" + 0.028*\"wage\" + 0.026*\"employment\" + 0.026*\"delta\" + '\n",
      "  '0.023*\"conflict\" + 0.018*\"wing\"'),\n",
      " (5,\n",
      "  '0.100*\"impose\" + 0.013*\"musical\" + 0.006*\"wong\" + 0.005*\"abbott\" + '\n",
      "  '0.003*\"conductor\" + 0.001*\"enlist\" + 0.001*\"piano\" + 0.000*\"braille\" + '\n",
      "  '0.000*\"sec\" + 0.000*\"reporting\"'),\n",
      " (6,\n",
      "  '0.046*\"year\" + 0.025*\"rise\" + 0.025*\"say\" + 0.020*\"market\" + 0.017*\"fall\" + '\n",
      "  '0.016*\"growth\" + 0.016*\"month\" + 0.015*\"analyst\" + 0.015*\"profit\" + '\n",
      "  '0.014*\"company\"'),\n",
      " (7,\n",
      "  '0.039*\"film\" + 0.036*\"disney\" + 0.031*\"game\" + 0.029*\"award\" + '\n",
      "  '0.026*\"director\" + 0.018*\"tv\" + 0.015*\"auto\" + 0.015*\"video\" + 0.012*\"slip\" '\n",
      "  '+ 0.012*\"leicester\"'),\n",
      " (8,\n",
      "  '0.093*\"recovery\" + 0.079*\"investigation\" + 0.046*\"river\" + '\n",
      "  '0.043*\"negotiate\" + 0.038*\"crisis\" + 0.032*\"examine\" + 0.020*\"scrutiny\" + '\n",
      "  '0.016*\"attitude\" + 0.016*\"relief\" + 0.008*\"belgian\"'),\n",
      " (9,\n",
      "  '0.036*\"leadership\" + 0.023*\"siemen\" + 0.018*\"eu\" + 0.011*\"mep\" + '\n",
      "  '0.010*\"referendum\" + 0.010*\"advocate\" + 0.009*\"ibm\" + 0.008*\"patent\" + '\n",
      "  '0.008*\"running\" + 0.006*\"lenovo\"'),\n",
      " (10,\n",
      "  '0.030*\"second\" + 0.027*\"first\" + 0.020*\"final\" + 0.020*\"match\" + '\n",
      "  '0.019*\"france\" + 0.018*\"set\" + 0.017*\"last\" + 0.017*\"half\" + 0.016*\"world\" '\n",
      "  '+ 0.015*\"top\"'),\n",
      " (11,\n",
      "  '0.073*\"music\" + 0.033*\"phone\" + 0.031*\"album\" + 0.031*\"mobile\" + '\n",
      "  '0.031*\"technology\" + 0.019*\"uncertainty\" + 0.019*\"network\" + '\n",
      "  '0.017*\"release\" + 0.015*\"electronic\" + 0.014*\"illegally\"'),\n",
      " (12,\n",
      "  '0.018*\"say\" + 0.017*\"china\" + 0.014*\"make\" + 0.013*\"england\" + '\n",
      "  '0.013*\"country\" + 0.013*\"world\" + 0.011*\"also\" + 0.011*\"india\" + '\n",
      "  '0.009*\"export\" + 0.009*\"work\"'),\n",
      " (13,\n",
      "  '0.063*\"import\" + 0.022*\"gun\" + 0.014*\"presentation\" + 0.014*\"impression\" + '\n",
      "  '0.013*\"stable\" + 0.012*\"widely_expecte\" + 0.011*\"extensive\" + '\n",
      "  '0.011*\"behind\" + 0.008*\"awesome\" + 0.008*\"fifa\"'),\n",
      " (14,\n",
      "  '0.124*\"sale\" + 0.089*\"bank\" + 0.044*\"sell\" + 0.038*\"unit\" + 0.022*\"store\" + '\n",
      "  '0.019*\"index\" + 0.019*\"stock\" + 0.017*\"giant\" + 0.017*\"buy\" + 0.015*\"asia\"'),\n",
      " (15,\n",
      "  '0.020*\"go\" + 0.019*\"good\" + 0.017*\"say\" + 0.016*\"get\" + 0.013*\"well\" + '\n",
      "  '0.013*\"year\" + 0.012*\"play\" + 0.012*\"time\" + 0.011*\"take\" + 0.011*\"do\"'),\n",
      " (16,\n",
      "  '0.047*\"thoma\" + 0.027*\"pleaded_guilty\" + 0.022*\"agent\" + 0.019*\"conspiracy\" '\n",
      "  '+ 0.017*\"count\" + 0.008*\"raid\" + 0.007*\"fbi\" + 0.006*\"berlin\" + '\n",
      "  '0.004*\"tower\" + 0.001*\"peer_network\"'),\n",
      " (17,\n",
      "  '0.106*\"loan\" + 0.089*\"union\" + 0.062*\"indonesia\" + 0.058*\"sue\" + '\n",
      "  '0.038*\"offset\" + 0.019*\"israeli\" + 0.011*\"continental\" + 0.010*\"suspicion\" '\n",
      "  '+ 0.008*\"transparent\" + 0.006*\"contractor\"'),\n",
      " (18,\n",
      "  '0.092*\"fraud\" + 0.064*\"card\" + 0.013*\"id_card\" + 0.010*\"proportion\" + '\n",
      "  '0.008*\"theft\" + 0.008*\"identity\" + 0.008*\"d\" + 0.004*\"newton\" + '\n",
      "  '0.003*\"identical\" + 0.003*\"privacy\"'),\n",
      " (19,\n",
      "  '0.072*\"pledge\" + 0.018*\"dent\" + 0.013*\"monitoring\" + 0.009*\"idle\" + '\n",
      "  '0.007*\"hole\" + 0.006*\"satisfied\" + 0.003*\"offence\" + 0.003*\"outlaw\" + '\n",
      "  '0.003*\"garden\" + 0.002*\"screensaver\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -12.948685505179276\n",
      "\n",
      "Coherence Score:  0.4468473887275787\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.update({'MALLET_HOME':r'/home/ila/Documents/repos/python-works/artificialintelligence/topic_modelling/mallet-2.0.8/bin/mallet'})\n",
    "mallet_path = '/home/ila/Documents/repos/python-works/artificialintelligence/topic_modelling/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=16, id2word=id2word)\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics = \", m, \" has Coherence Value of\", round(cv, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
